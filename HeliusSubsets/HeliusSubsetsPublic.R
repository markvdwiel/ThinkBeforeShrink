#' # Think before you shrink
#' 
#' Script: Mark van de Wiel, mark.vdwiel@amsterdamumc.nl;
#' Synthetic data generated by Jeroen Hoogland, j.hoogland@amsterdamumc.nl;
#' Package 'shrinkage', version 1.2, written by GwenaÃ«l Leday, g.g.r.leday@wur.nl
#' 
#' Script allows for repeating calculations as performed in the Manuscript:
#' "Think before you shrink: Alternatives to default shrinkage methods can 
#' improve prediction accuracy, calibration and coverage" on a synthetic 
#' copy of the data provided with this script. Note: results may deviate 
#' somewhat from those in the manuscript, as the latter result from 
#' the real data. This script presents results for linear regression. 
#' Results for logistic regression on simulated data are presented in the 
#' LogisticSimPublic.R script
#'  
#' Script evaluates predictive performance in terms of accuracy, calibration 
#' and coverage for several shrinkage methods, including some Bayesian ones.
#' Focus lies on variations of ridge-type shrinkage.
#'
#' ## Preliminaries
library(mgcv) #available from CRAN
library(glmnet) #available from CRAN

#' install shrinkage package from github run:
#' library(devtools); install_github("gleday/shrinkage") 
#' OR if you haven't installed devtools yet, run:
#' install.packages("devtools"); library(devtools); 
#' Alternatively (without devtools), download the entire folder of source material 
#' from GitHub repo gleday/shrinkage and install from source using
#' install.packages("[path]/shrinkage-master/", repos=NULL, type="source")
library(shrinkage) 


#' Source functions
#setwd() #to change working directory
source('HeliusSubsetsPublic_functions.R')

#' One may skip the most time-consuming part, fitting of the models, if
#' these are already available 
dofit <- F

#' Set path for loading data and storing results (ADJUST)
wd <- "C:\\ExternData\\Helius\\synthetic"

#' Load synthetic data
setwd(wd)
load("d1.Rdata") 

#' Rename for convenience
helius4 <- dsynth

#' add 5 noise variables
addnoise <- TRUE 
set.seed(34623)
noisedf <- data.frame(matrix(rnorm(5*nrow(helius4)),ncol=5))
colnames(noisedf) <- paste("Noise",1:5)
#save(helius4,noisedf, file="d1noise.Rdata") #save for reproducibility
if(addnoise) helius4 <- data.frame(helius4,noisedf)

#' Nr of covariates; first one is the response variable (sbp)
ncov<- ncol(helius4[,-1])
ncov

nsam <- nrow(helius4)
nsam

#' Linear model on entirel data set
lmsbp <- lm(sbp ~ ., data = helius4)
summary(lmsbp)
effsize <- coef(lmsbp)
effsize[14:18] <- 0 #noise 
predtrue <- predict(lmsbp)
cn <- colnames(helius4)
cn
Y <- helius4$sbp
X <- helius4[,-1] #remove response

#' Creates a large test set
set.seed(2153524)
testfixedind <- sort(sample((1:nrow(helius4)),2000))


#' ## Sample splitting

#' Function to make data chuncks
makeTraining <- function(chunksize,nfit=25,nrepeat=1){
Y <- helius4$sbp
X <- helius4[,-1] #remove response
ssize <- length(Y)
ssize
ncov <- ncol(X)
nset <- floor(ssize/chunksize)
nset
set.seed(34634)
sets <- CVfolds(Y,model="linear", kfold=nset,nrepeat=nrepeat)
if(nfit >= length(sets)) {print("increase nrepeat > nfit * chunksize/N");return(NULL)}

#nfit <- 25
set.seed(2152)
nsetsfit <- sample(1:length(sets),nfit)


trainind <- lapply(nsetsfit, function(ind){setind <- sets[[ind]]; return(setind)})
return(trainind)
}

#' Create 400 subsets of sizes n=100, 400
td100_400 <- makeTraining(chunksize=100,nfit=400,nrepeat=2)
td200_400_4 <- makeTraining(chunksize=200,nfit=400,nrepeat=4)



#' ## Apply fitting methods
#' Fitting models for 400 (!) training data sets; n=100, n=200
#' Will take considerable time (several hours)

if(dofit){
  nfit <- length(td100_400)
  
  #' Variations of ridge with mgcv
  system.time(penCmgcv <- lapply(1:nfit,penCompare,trainind = td100_400))
  
  #' lasso with glmnet
  system.time(penClasso <- lapply(1:nfit,penComparelasso,trainind = td100_400))
  
  #' Variations of Bayesian ridge (including local) with shrinkage package
  #' Note: credibility intervals of predictions are computed on fixed test set
  #' when fitting to save memory space
  system.time(penCbayes <- lapply(1:nfit,penCompareBayes,trainind = td100_400,
                                  testind = testfixedind)) 
  save(penCmgcv,penClasso,penCbayes,file="resn100noise_nfit400.Rdata")
  
  #' Same for n=200
  nfit <- length(td200_400_4)
  system.time(penCmgcv <- lapply(1:nfit,penCompare,trainind = td200_400_4))
  system.time(penClasso <- lapply(1:nfit,penComparelasso,trainind = td200_400_4))
  system.time(penCbayes <- lapply(1:nfit,penCompareBayes,trainind = td200_400_4,testind = testfixedind)) 
  save(penCmgcv,penClasso,penCbayes,file="resn200noise_nfit400.Rdata")
  } 
  


#' ## Extracting results

nsub <- 100
setwd(wd)
if(nsub ==100) load("resn100noise_nfit400.Rdata") else load("resn200noise_nfit400.Rdata")

#' Extract coefficients from various fitters
coefsmgcv <- lapply(penCmgcv,coeffun,method="mgcv")
coefslasso <- lapply(penClasso,coeffun,method="glmnet")
coefsbayes <- lapply(penCbayes,coeffun,method="shrinkage")


#' ### Evaluating prediction accuracy 

Y <- helius4$sbp
X <- helius4[,-1] #remove response
if(nsub ==100) sets <- td100_400 else sets <- td200_400_4

#' Prediction squared error
rpsemgcv <- t(RPSE(coefsmgcv,sets=sets))
rpselasso <- t(RPSE(coefslasso,sets=sets))
rpsebayes<- t(RPSE(coefsbayes,sets=sets))

allrpse <- cbind(rpsemgcv[,1:2],rpselasso,rpsemgcv[,-(1:2)],rpsebayes)
dim(allrpse)
pseplot <- allrpse[,-14]
colnames(pseplot)
colnames(pseplot) <- c("OLS","step","lasso","ridge","ridge_2","ridge_2r","ridge_2un","ridge2_unr",
                       "ridge_3","Bay_EB","Bay_IG","Bay_glo","Bay_2","Bay_loc","Bay_3" )

#' Prediction MSE of various methods 
cn <- colnames(pseplot)
par(las=2,xaxt="n")
boxplot(pseplot,ylim =c(0,1.3),main="MSEp")
par(las=2,xaxt="s")
axis(1,at=1:length(cn),cn)
  

#' Comparing performance of 2-pen, 1-pen and 0-pen (OLS) for various sample 
#' sizes
nseq <- seq(100,200,by=15)
if(dofit){
  system.time(allfits <- lapply(nseq,pencompare3n,nfit=100))
  save(allfits, file="allfits_samplesize.Rdata")
  } else {
  setwd(wd)
  load("allfits_samplesize.Rdata")
  }

allmedians <- c()
for(k in 1:length(nseq)){
  print(nseq[k])
  fits <- allfits[[k]]
  coefs <- lapply(fits,coeffun,method="mgcv")
  nfit <- length(coefs)
  sets <- makeTraining(nseq[k],nfit=nfit)
  psemgcv <- t(RPSE(coefs,sets=sets))
  colnames(psemgcv) <- c("OLS","ridge","ridge_2")
  medians <- apply(psemgcv,2,median)
  allmedians<-rbind(allmedians,medians)
}
allmedians


plot(nseq,allmedians[,1],pch=1,type="b",ylim=c(0,allmedians[1,1]),xlab="",ylab="",lwd=3)
points(nseq,allmedians[,2],lty=1,col=2,pch=2,type="b",lwd=3)
points(nseq,allmedians[,3],lty=1,col=3,pch=3,type="b",lwd=3)
legend(165,0.35,legend=c("OLS","ridge","ridge_2"),col=1:3,lty=1,lwd=3,pch=1:3)

#' ### Evaluating Calibration (cslopes)
nsub <- 100
if(nsub == 100) sets <- td100_400 else sets <- td200_400_4

#' Compute calibration slopes per subset and method. Takes a minute. Regression
#' slopes ypred vs true can be obtained by replacing 'calibrateperset0'
#' by 'calibrateperset'
cbbayset <- calibrateperset0(coefsbayes,sets,c(3,4,6))
cbset <- calibrateperset0(coefsmgcv,sets,c(1,3,4))

slopemat0 <- cbind(cbset,cbbayset)
slopemat<- slopemat0[,c(1,6,2,4,3,5)]
colnames(slopemat) <- c("OLS", "Bay_loc","ridge","Bay_glo","ridge_2","Bay_2")

par(mar=c(5,4,4,3))
boxplot(slopemat,xlab="",main="")
abline(h=1)


#' rmses of the slopes
for(k in 1:6) print(sqrt(mean((slopemat[,k]-1)^2)))

#' ### Explaining (mis)-calibration: coefficient of BMI

#' Plot coefficients and the truth for BMI
coefBMIOLS <- unlist(lapply(coefsmgcv,function(mat) mat[11,1]))
coefBMIridge <- unlist(lapply(coefsmgcv,function(mat) mat[11,3]))
coefBMIBayGlo <- unlist(lapply(coefsbayes,function(mat) mat[11,3]))
coefBMIBayLoc <- unlist(lapply(coefsbayes,function(mat) mat[11,6]))
coefsBMI <- cbind(coefBMIOLS,coefBMIBayLoc,coefBMIridge,coefBMIBayGlo)

par(mar=c(5,4,4,3))
colnames(coefsBMI) <-  c("OLS","Bay_loc","ridge","Bay_glo")
boxplot(coefsBMI)
abline(h=effsize[11])

#' Extremes for OLS
maxols <- which.max(coefBMIOLS)
minols <- which.min(coefBMIOLS)
minmax <- c(minols,maxols)

#' Illustrate shrinkage on BMI coefficient
nsam <- 100000
rnormcauchy <- rnorm(nsam,0,sd=abs(rcauchy(nsam)))
allrnc <- c(rnormcauchy,-rnormcauchy)
wh <- which(abs(allrnc)<=10)
par(mar=c(5,4,4,3))
plot(density(allrnc[wh]),xlim=c(-2,2),xlab="",ylab="",main="")
abline(v=coefBMIOLS[minols],lty=3,lwd=2)
abline(v=coefBMIBayLoc[minols],lty=2,lwd=2)
abline(v=coefBMIOLS[maxols],lty=3,lwd=2)
abline(v=coefBMIBayLoc[maxols],lty=2,lwd=2)
arrows(coefBMIOLS[minols]+0.03, 0.05, x1 = coefBMIBayLoc[minols] -0.03, y1 = 0.05,length=0.1,lwd=2)
arrows(coefBMIOLS[maxols]-0.03, 0.05, x1 = coefBMIBayLoc[maxols] +0.03, y1 = 0.05,code=2,length=0.1,lwd=2)
abline(v=effsize[11],lwd=2)


#' ### Variability of ridge penalties
#' 
#' Assumes nsub is set (to 100 or 200) and fitting results have been loaded
#' (see above)
#' 
#' Retrieve ridge penalties from mgcv results
pensmgcv <- t(sapply(1:length(penCmgcv),
                   function(i){
                     penC1 <- penCmgcv[[i]]
                     pen1mml <- penC1$pred1$sp
                     pen2mml <- penC1$pred2$sp
                     return(c(pen1mml,pen2mml))
                   }
                   ))
colnames(pensmgcv) <- c("pen1mml","pen2mml_1","pen2mml_2")

#' Retrieve penalties from Bayes global (posterior mode)
pensbayglo <- 1/t(sapply(1:length(penCbayes),
                       function(i){
                         #i<-1
                         penC1 <- penCbayes[[i]]
                         pens<-sapply(c(1,3),function(i){ex <- penC1[[i]]; 
                         if(i==1) return(ex$tau2s_summary[1]) else return(ex$tau2s_summary[8])
                         })
                         return(pens)
                       }
))
colnames(pensbayglo) <-  c("Bay_ridgeMML","Bay_glo")

#' Retrieve penalties from Bay_2 (posterior mode)
pensbay2 <- 1/t(sapply(1:length(penCbayes),
                         function(i){
                           penC1 <- penCbayes[[i]]
                           pens<-sapply(c(4),function(i){ex <- penC1[[i]]; 
                           if(i==1) return(ex$tau2s_summary[1]) else return(ex$tau2s_summary[2:3,8])
                           })
                           return(pens)
                         }
))

#' Retrieve posterior intervals
penintervals <- 1/t(sapply(1:length(penCbayes),
                      function(i){
                        inter <- penCbayes[[i]][[3]]$tau2s_summary[c(6,5,4)]
                        return(inter)
                      }
                        ))

pensboth <- cbind(pensmgcv[,1],pensbayglo[,2])

#' Plot variability of penalties; between subsets (left-side), within 
#' subsets (right side; Bayesian estimate)
par(las=2,xaxt="n",mar=c(5,4,4,3))
for(i in 1:2){
  #i<-1
  pb <- log(pensboth[,i])
mediani <- median(pb)
q025i <- quantile(pb,probs=c(0.025,0.975))
if(i==1) plot(i,mediani, cex=2, pch=16, xlim=c(0.5,13),xlab="",  ylim=c(0,8),
              ylab="") else points(i,mediani, cex=2, pch=16)
segments(i,q025i[1],i,q025i[2],lty=2)
}
sqint <- log(penintervals)
set.seed(34534)
set10 <- sample(1:nrow(sqint),10)
for(j in 1:10){
  indj <- set10[j]
  modej <- sqint[indj,2] 
  points(j+3,modej, cex=2, pch=16, col="grey")
  segments(j+3,sqint[indj,1],j+3,sqint[indj,3],lty=2,col="grey")
  
  
}
abline(v=3)
par(las=2,xaxt="s")
axis(1,at=c(1:2,4:13),c("ridge", "Bay_glo",paste("subset",1:10)))

#' Plot variability of penalties, two covariate group setting. Note:
#' missings may occur, but are rare, so ignored
pensfour <- cbind(pensmgcv[,2],pensbay2[,2],pensmgcv[,3],pensbay2[,1])
par(las=2,xaxt="n",mar=c(7,4,4,3))
for(i in 1:4){
  #i<-1
  pb <- log(pensfour[,i])
  mediani <- median(pb,na.rm=T)
  q025i <- quantile(pb,probs=c(0.025,0.975),na.rm=T)
  if(i==1) plot(i,mediani, cex=2, pch=16, xlim=c(0.5,4),xlab="",  ylim=c(0,11),ylab="penalty (log)") else points(i,mediani, cex=2, pch=16)
  #points(i,mediani+sdi,cex=2,pch=25,bg="grey")
  segments(i,q025i[1],i,q025i[2],lty=2)
  abline(v=2.5)
  par(las=2,xaxt="s")
  axis(1,at=1:4,c("ridge_2_lam1", "Bay_2_lam1","ridge_2_lam2", "Bay_2_lam2"))
}

#' ## Coverage of confidence intervals of predictions
#' 
#' Assumes nsub is set (to 100 or 200) and fitting results have been loaded
#' (see above)
#' 
nfit <- length(penCmgcv)

#' Coverage test sets predictions for OLS, ridge, ridge2 per training data set
coverridges <- sapply(c(1,3,4),covermgcv,fit=penCmgcv,uncond = T,bench=predtrue,testind=testfixedind)
#' Quantiles across data sets of coverages of OLS, ridge, ridge2
coversr <- apply(coverridges,2,quantile,probs=c(0.1,0.5,0.9))
colnames(coversr) <- c("OLS","ridge", "ridge2")
coversr
covmeanr <- apply(coverridges,2,mean)

#' Coverage test sets predictions for Bay_loc, Bay_glo, Bay2 per training data set
coverbayess <- sapply(c(6,3,4),coverbayes,fit=penCbayes, testind=testfixedind, bench=predtrue)
#' Quantiles across data sets of coverages of Bay_loc, Bay_glo, Bay2
coversb <- apply(coverbayess,2,quantile,probs=c(0.1,0.5,0.9))
colnames(coversb) <- c("Bay_loc","Bay_glo", "Bay2")
coversb
covmeanb <- apply(coverbayess,2,mean)

#' Widths of intervals for OLS, ridge, ridge2 per training data set
widthridges <- sapply(c(1,3,4),widthmgcv,fit=penCmgcv,uncond = T,bench=predtrue,testind=testfixedind)
widthsr <- apply(widthridges,2,quantile,probs=c(0.1,0.5,0.9))
colnames(widthsr) <- c("OLS","ridge", "ridge2")
widthsr
widmeanr <- apply(widthridges,2,mean)

#' Widths of intervals for Bay_loc, Bay_glo, Bay2 per training data set
widthbayess <- sapply(c(6,3,4),widthbayes,fit=penCbayes, testind=testfixedind, bench=predtrue)
widthsb <- apply(widthbayess,2,quantile,probs=c(0.1,0.5,0.9))
colnames(widthsb) <- c("Bay_loc","Bay_glo", "Bay2")
widthsb
widmeanb <- apply(widthbayess,2,mean)

#' Summary table comparing frequentist (F) results with its Bayesian (B) 
#' counterpart
allmean <- round(cbind(covmeanr,covmeanb,widmeanr,widmeanb),3)
rownames(allmean) <- c("OLS (F) and Bay_loc (B)","ridge (F) and Bay_glo (B)", "ridge2 (F) and Bay2 (B)")
colnames(allmean) <- c("Cov,F", "Cov,B", "Width,F", "Width,B")

allmean



